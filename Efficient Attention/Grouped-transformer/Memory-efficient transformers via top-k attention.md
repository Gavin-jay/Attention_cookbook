> # Memory-efficient transformers via top-k attention
>
> we propose top-k attention, where for each query, we mask out all but its k largest dot products with the keys, that is, in each row of QK> we only keep its k largest elements and mask out the rest.

> * While these variants are memory and compute efficient, it is not possible to directly use them with popular pre-trained language models trained using vanilla attention, without an expensive corrective pre-training stage
> * In this work, we propose a simple yet highly accurate approximation for vanilla attention. We process the queries in chunks, and for each query, compute the top-k scores with respect to the keys. Our approach offers several advantages: (a) its memory usage is linear in the input size, similar to linear attention variants, such as Performer and RFA (b) it is a drop-in replacement for vanilla attention that does not require any corrective pre-training, and (c) it can also lead to significant memory savings in the feed-forward layers after casting them into the familiar query-keyvalue framework.
> * We show that top-k attention can be implemented in a memory-efficient manner by (a) chunking the query vectors when computing the output one chunk at a time, when computing softmax(QK>)V , and (b) a custom implementation of the forward and backward pass that does not require caching activations while processing chunks in the forward pass.
>
> ## Related work
>
> * To alleviate this, past work proposed approximation methods for the computation of softmax(QK>). One major line of research focused on sparse attention variants, where only a few similarity scores are computed per query, and the rest are ignored. Methods differ by which query-key pairs are selected [5, 52, 32, 40, 21, 2, 15, 49]. A second line of research explored dense variants [19, 50, 1, 44] (cf. [46] for a survey). For example, instead of computing the attention scores exactly for only a small number of query-key pairs, [6] compute an approximation of scores for all pairs.
>
> ## Detail
>
> * In this work, we adopt the sparse attention approach, but rather than approximating the k most similar key vectors per query vector, we compute this quantity exactly.
> * Compared to prior methods, top-k attention has multiple attractive properties:  • Top-k attention has the same memory footprint as Performer [6], a state-of-the-art attention variant with linear time and memory complexity, on very long inputs (orange curve, Fig. 1, top-right), while being as fast as vanilla attention, and even faster than linear variants on inputs of length up to 4K (Figure 1, bottom-left). This allows us, e.g., to train a typical 12-layer Transformer decoder over 32K-long inputs on a 30GiB GPU (Figure 3a).
> * Top-k attention also reduces memory consumption in Transformer feed-forward layers, by casting this layer into the familiar query-key-value framework using ReLU instead of the row-wise softmax [42]. This is specifically appealing in models such as T5 [34] and GPT-3 [3], where for short inputs, the memory consumption is dominated by the feed-forward layers, as the number of keys, corresponding to the feed-forward hidden dimension size, is as large as 65K. Conversely, methods that rely on random feature approximations of attention, such as Performer [6] and RFA [29] do not admit an efficient approximation for the ReLU activation [53].  • Top-k attention is a highly accurate approximation to vanilla attention and is a plug-and-play replacement at both multi-head attention and feed-forward layers of a Transformer. This is unlike past attention variants [19, 6, 29] that require an expensive corrective pre-training stage to adjust model weights to the new variant, which can be prohibitive for large models. We show top-k attention can replace vanilla attention in a zero-shot inference setup and at fine-tuning time without any corrective pre-training
> * In the common case where the input sequences are relatively short, memory consumption is dominated by the feed-forward sub-layer and not the self-attention sub-layer.
